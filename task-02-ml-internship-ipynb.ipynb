{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install packages\n!pip install unsloth --upgrade\n!pip install wandb\n\n# Torch imports\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# Import wandb safely\nimport os\nimport wandb\n\n# Disable wandb online mode (no login needed)\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\nprint(\"Wandb tracking is disabled. Continuing without wandb login.\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:22:31.997469Z","iopub.execute_input":"2025-04-26T19:22:31.997784Z","iopub.status.idle":"2025-04-26T19:25:57.347624Z","shell.execute_reply.started":"2025-04-26T19:22:31.997761Z","shell.execute_reply":"2025-04-26T19:25:57.346302Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fcdee81cd90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/unsloth/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fcdee78bc50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/unsloth/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fcdee75b250>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/unsloth/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fcdee733510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/unsloth/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fcdee771550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/unsloth/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nUsing device: cpu\nWandb tracking is disabled. Continuing without wandb login.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# PART 2: Dataset Preparation\nLetâ€™s load and process the dataset.the FreedomIntelligence/Medical-CoT dataset.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"FreedomIntelligence/Medical-CoT\")\ndf = dataset['train'].to_pandas()\n\n# Format <think> and <response> tags\ndef format_sample(row):\n    return f\"<think>{row['rationale']}</think>\\n<response>{row['answer']}</response>\"\n\ndf[\"formatted\"] = df.apply(format_sample, axis=1)\n\n# Train/Validation Split\ntrain_data = df.iloc[100:]\nval_data = df.iloc[:100]\n\ntrain_texts = train_data[\"formatted\"].tolist()\nval_texts = val_data[\"formatted\"].tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load LLaMA 3.2 (3B) Quantized & Setup LoRA","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-3b-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = torch.float16,\n    load_in_4bit = True,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 32,\n    lora_dropout = 0.05,\n    bias = \"none\",\n    task_type = \"CAUSAL_LM\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset to Dataloader Format","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom unsloth.data import preprocess\n\ntrain_dataset = preprocess(train_texts, tokenizer)\nval_dataset = preprocess(val_texts, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune with wandb Tracking","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_PROJECT\"] = \"llama3-medical-finetune\"\n\ntraining_args = TrainingArguments(\n    output_dir = \"./llama3-cot-finetuned\",\n    per_device_train_batch_size = 2,\n    per_device_eval_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    evaluation_strategy = \"epoch\",\n    logging_strategy = \"steps\",\n    logging_steps = 20,\n    num_train_epochs = 3,\n    learning_rate = 2e-4,\n    fp16 = True,\n    report_to = \"wandb\",\n    save_strategy = \"no\",\n)\n\ntrainer = Trainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation using ROUGE-L","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric\n\nrouge = load_metric(\"rouge\")\n\ndef compute_rouge(preds, targets):\n    results = rouge.compute(predictions=preds, references=targets)\n    return results[\"rougeL\"].mid.fmeasure\n\n# Example evaluation\nsample_preds = [tokenizer.decode(model.generate(tokenizer(text, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=100)[0]) for text in val_texts[:10]]\nsample_targets = [text.split(\"<response>\")[1].replace(\"</response>\", \"\") for text in val_texts[:10]]\n\nrouge_l_score = compute_rouge(sample_preds, sample_targets)\nprint(\"ROUGE-L:\", rouge_l_score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save and Upload to Hugging Face","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"finetuned-lora\")\ntokenizer.save_pretrained(\"finetuned-tokenizer\")\n\n# Then, in a cell:\n# Login to Hugging Face CLI to upload\n# !huggingface-cli login\n\n# After login:\n# !huggingface-cli repo create llama3-medical-cot --type model\n# !git clone https://huggingface.co/username/llama3-medical-cot\n# !cp -r finetuned-lora/* llama3-medical-cot/\n# !cp -r finetuned-tokenizer/* llama3-medical-cot/\n# !cd llama3-medical-cot && git add . && git commit -m \"Upload finetuned model\" && git push\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Instructions","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-3b-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = torch.float16,\n    load_in_4bit = True,\n)\n\nmodel.load_adapter(\"path_to_your_finetuned_lora\")\n\nprompt = \"<think>Patient shows signs of...</think> <response>\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\noutput = model.generate(input_ids, max_new_tokens=100)\nprint(tokenizer.decode(output[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"end of task","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}